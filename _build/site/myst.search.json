{"version":"1","records":[{"hierarchy":{"lvl1":"Cytosol Analysis"},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":"Cytosol Analysis"},"content":"Tools for working with data from bulk cytosol assays, particularly those performed on plate readers.","type":"content","url":"/","position":1},{"hierarchy":{"lvl1":"Cytosol Analysis","lvl2":"Cytosol Kinetics"},"type":"lvl2","url":"/#cytosol-kinetics","position":2},{"hierarchy":{"lvl1":"Cytosol Analysis","lvl2":"Cytosol Kinetics"},"content":"Analyzes kinetics of expression as read by a plate reader, and outputs data and plots describing key kinetic parameters of each reaction including lag time, maximum velocity, and the steady-state maximum expression level.","type":"content","url":"/#cytosol-kinetics","position":3},{"hierarchy":{"lvl1":"Timecourse with interactivity"},"type":"lvl1","url":"/interactivity","position":0},{"hierarchy":{"lvl1":"Timecourse with interactivity"},"content":"","type":"content","url":"/interactivity","position":1},{"hierarchy":{"lvl1":"Timecourse with interactivity","lvl3":"Interactivity"},"type":"lvl3","url":"/interactivity#interactivity","position":2},{"hierarchy":{"lvl1":"Timecourse with interactivity","lvl3":"Interactivity"},"content":"\n\n","type":"content","url":"/interactivity#interactivity","position":3},{"hierarchy":{"lvl1":"Timecourse with interactivity","lvl2":"Time course measurements"},"type":"lvl2","url":"/interactivity#time-course-measurements","position":4},{"hierarchy":{"lvl1":"Timecourse with interactivity","lvl2":"Time course measurements"},"content":"A shared understanding of how to interpret data from CFE systems would speed the development of measurements and standards towards improved reproducibility and address challenges in interpreting and comparing existing and future data, including data from failed experiments. Workshop participants agreed that time-course measurements of the product expressed in the CFE reaction should be favored over endpoint measurements, despite the time, labor, and costs involved, to obtain a more complete and informative view of a CFE reaction. Whenever possible, measurements should be reported as a reduced quantity, such as a mean value, with uncertainty (Figure 2A) and include a baseline from negative control measurements. Several recent studies have commented on metrics available from a time-course measurement of protein expression in a CFE workflow, for example, for the purpose of reaction optimization [ref] and the development of predictive modeling tools [ref]. Workshop participants emphasized several of these metrics, displayed below in Figure 2\n\nMaximum yield of product expressed;\n\nt_{max} Time to reach the maximum yield of product expressed, as the time from the start of the measurement to the time to reach the maximum yield;\n\n v_{max} Maximum rate of product expression, as the maximum linear rate of production;\n\nLag time, as the time from the start of the measurement to the time to reach the maximum rate of expression;\n\nInflection time, as the time from the start of the measurement to the time at which the rate of product expression begins to decrease;\n\nPercent decline, as a decrease in the amount of product expressed from the maximum yield; and,\n\nTime to percent decline, as the amount of time from the start of the measurement to the time to reach a predefined decrease in the yield of product expressed after having reached its maximum yield of product expressed.","type":"content","url":"/interactivity#time-course-measurements","position":5},{"hierarchy":{"lvl1":"Timecourse with interactivity","lvl3":"Workflow","lvl2":"Time course measurements"},"type":"lvl3","url":"/interactivity#workflow","position":6},{"hierarchy":{"lvl1":"Timecourse with interactivity","lvl3":"Workflow","lvl2":"Time course measurements"},"content":"","type":"content","url":"/interactivity#workflow","position":7},{"hierarchy":{"lvl1":"Timecourse with interactivity","lvl4":"Platemap","lvl3":"Workflow","lvl2":"Time course measurements"},"type":"lvl4","url":"/interactivity#platemap","position":8},{"hierarchy":{"lvl1":"Timecourse with interactivity","lvl4":"Platemap","lvl3":"Workflow","lvl2":"Time course measurements"},"content":"Note\n\nIdeally, we would have a tool that (1) let’s people describe experiments in a standardized way (spec lang), (2) that can then be mapped onto a platemap, and (3) displayed in part or whole in this section [this might be an interesting place for a widget to explore the parameters of the experiment]\n\nThis is text describing the context of the experiment. There a few scenarios where this text can end up here. We can imagine users working in jupyterhub typing directly or its copy and pasted in from an experimental notebook. In this particular case, we have two experiments on testing the effect of DNA concentration and a second experiment comparing different DNA constrcuts - the infamous artifact 11 (AR-11)\n\nimport data.fusion_utils as utils\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\nimport ipywidgets as widgets\nfrom ipywidgets import interact\n\n# LOAD YOUR PLATEMAP HERE\"\n# platemap_path = \"./sample-data/platemap.txt\"\n# platemap = read_platemap_tsv(platemap_path)\n\n# Create a dropdown widget to control the table\nmenu = widgets.Dropdown(\n    options=['DNA Comparison', 'DNA Concentration'],\n    value='DNA Concentration',\n    description='Experiment:',\n)\n\n# Define a function that will plot a sine wave based on the frequency\ndef update_plot(exp_ID):\n    # for cleaner visualization\n    # slx_platemap = platemap.set_index('Well', drop=True, inplace=False)\n    # show_platemap = slx_platemap[slx_platemap[\"Experiment\"] == exp_ID].drop(columns=['Blank', 'Row', 'Column'])\n\n    # show_platemap_length = len(show_platemap)\n    # display(show_platemap.head(show_platemap_length))\n    if exp_ID == \"DNA Comparison\":\n        img = mpimg.imread('./figs/Figure_1_math.png')  # Replace 'your_image.png' with the path to your PNG file\n        # Display the image\n        plt.imshow(img)\n        plt.axis('off')  # Optional: Turns off axis labels for a cleaner display\n        plt.show()\n\n# Use interact to connect the slider to the update_plot function\ninteract(update_plot, exp_ID=menu)\ndisplay()\n\n\n\nWarning\n\nThe Binder hub behind the scenes is associated with an article from the Journal Elemental Microscopy.\n\nSee all experiments\n\nTable 1:Experimental conditions studied","type":"content","url":"/interactivity#platemap","position":9},{"hierarchy":{"lvl1":"Timecourse with interactivity","lvl4":"Analysis","lvl3":"Workflow","lvl2":"Time course measurements"},"type":"lvl4","url":"/interactivity#analysis","position":10},{"hierarchy":{"lvl1":"Timecourse with interactivity","lvl4":"Analysis","lvl3":"Workflow","lvl2":"Time course measurements"},"content":"One thing that might be interesting to show here would be a progression from end-point analysis, unannotated timeseries, time series annotations, and then exploring the kinetic parameters.\n\n# experiment -> \"DNA Concentration\"\ndata_concentration = data[data[\"Experiment\"] == \"DNA Concentration\"]\n\nProgram 1:filter the plate map to select for a specific experiment\n\nHere, \n\nProgram 1 is used to generate the following figure using these \n\nspecific lines of code from the cytosol analysis toolkit. This might be useful for instance if we’re showcasing a new feature in the codebase and we want to make a tight connection between code and function, inviting engagement.\n\nThe immediate advantage of this notebook over our existing way of publishing is that the figure is unambiguously linked to the data and code used to generate it. In this particular case, the sample data is co-located in the directory used to generate the HTML. Another approach might be to pull the data programmatically from an archive like zenodo.\n\nPulling data directly from an archive# this workflow would also work for OSF\nfrom zenodo_client import Zenodo\n\nzenodo = Zenodo(access_token=token)\nOOH_NA_NA_RECORD = '13852103'\n\nplatemap_file = '20240916-platemap.xlsx'\ndata_file = 'timecourse.csv'\n\ndata = zenodo.download_latest(OOH_NA_NA_RECORD, data_file)\nplatemap = zenodo.download_latest(OOH_NA_NA_RECORD, platemap_file)\n\n\n\nFigure 1:The plasmid pT7-deGFP (AR-11) is swept from 0-100 ng/μL in the PURE system\n\nNow, we can showcase the kinetic analysis tool by showing a single timeseries selected from the above experiment where the extracted features are annotated. It was a bit annoying to extract one plot from the FacetGrid. Instead, I reduced the DataFrame\n\nPlotting a single annotated kinetic tracewarnings.filterwarnings(\"ignore\")\n\nsingle_well = data_concentration[data_concentration[\"Well\"] == \"B1\"]\n\nsingle_well_kinetics = kinetic_analysis(data=single_well, data_column=\"Data\", time_cutoff=15000)\ng = sns.FacetGrid(single_well_kinetics, col=\"Well\", col_wrap=2, sharey=False, height=4, aspect=1.5)\ng.map_dataframe(plot_kinetics, y=\"Data\", show_fit=True, show_velocity=False, annotate=True)\n\n\n\nFigure 2:Annotated timeseries for pT7-deGFP (AR-11) in PURE at a concentration of 100 ng/μL\n\nOf course, the figure isn’t really what’s valuable. Ultimately, we want to associate the dimension-reduced kinetic parameters with the experimental specification. We can push this to a database like SeqBase where the input sequence and experimental specification are coupled to the output kinetic parameters.\n\nOther types of data for non-fluorescent data will have to be developed over time.\n\nTable 1:Combined experimental specification with kinetic parameters extracted from data.","type":"content","url":"/interactivity#analysis","position":11},{"hierarchy":{"lvl1":"Kinetic analysis notebook"},"type":"lvl1","url":"/kinetic-analysis-nb","position":0},{"hierarchy":{"lvl1":"Kinetic analysis notebook"},"content":"import io\nimport re\nimport glob\nimport datetime\nimport warnings #useful for having cleaner outputs that get rolled into .md files\nfrom functools import partial\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.optimize import curve_fit\n\n\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport seaborn as sns\n\n\n","type":"content","url":"/kinetic-analysis-nb","position":1},{"hierarchy":{"lvl1":"Kinetic analysis notebook","lvl3":"Load Platemap"},"type":"lvl3","url":"/kinetic-analysis-nb#load-platemap","position":2},{"hierarchy":{"lvl1":"Kinetic analysis notebook","lvl3":"Load Platemap"},"content":"\n\n# These should be rolled into a library\n\ndef read_platemap_tsv(platemap_path: str) -> pd.DataFrame:\n    platemap = pd.read_table(platemap_path)\n\n    platemap[\"Row\"] = platemap[\"Well\"].apply(lambda s: re.search(r\"[A-Z]\", s).group())\n    platemap[\"Column\"] = platemap[\"Well\"].apply(lambda s: int(re.search(r\"\\d+\", s).group()))\n    platemap[\"Column\"] = platemap[\"Column\"].astype(int)\n\n    return platemap\n\ndef read_platemap_str(platemap_str: str) -> pd.DataFrame:\n    platemap = pd.read_table(io.StringIO(platemap_str), index_col=0)\n    print(platemap)\n    platemap.index.name = \"Row\"\n    platemap = platemap.reset_index().melt(id_vars=[\"Row\"], var_name=\"Column\", value_name=\"Construct\")\n    \n    platemap[\"Column\"] = platemap[\"Column\"].astype(int)\n    platemap[\"Well\"] = platemap.apply(lambda well: f\"{well['Row']}:{well['Column']}\", axis=1)\n    platemap = platemap.rename(columns={\"Construct\": \"Label\"})\n    platemap = platemap.dropna()\n    return platemap\n\ndef read_platemap_excel(platemap_path: str) -> pd.DataFrame:\n    \"\"\"\n    Use like this:\n\n    \n    > platemap_path = \"bnext/experiments/20240703-PURE-pT7deGFP-tetR-lacI/20240703-PURE-validate-pT7-deGFP-tetR-lacI.xlsx\"\n    > platemap = read_platemap_excel(platemap_path)\n    > platemap.head()\n    \"\"\"\n    platemap = pd.read_excel(platemap_path)\n    platemap.fillna(value=0, inplace=True)\n    platemap['Row'] = platemap['Well'].apply(lambda s: s.split(\":\")[0])\n    platemap['Column'] = platemap['Well'].apply(lambda s: s.split(\":\")[1]).astype(int)\n\n    return platemap\n\n# LOAD YOUR PLATEMAP HERE\"\nplatemap_path = \"./sample-data/platemap.txt\"\nplatemap = read_platemap_tsv(platemap_path)\n\n# for cleaner visualization\nplatemap.set_index('Well', drop=True, inplace=False)\n\n","type":"content","url":"/kinetic-analysis-nb#load-platemap","position":3},{"hierarchy":{"lvl1":"Kinetic analysis notebook","lvl2":"Parse Data"},"type":"lvl2","url":"/kinetic-analysis-nb#parse-data","position":4},{"hierarchy":{"lvl1":"Kinetic analysis notebook","lvl2":"Parse Data"},"content":"\n\n","type":"content","url":"/kinetic-analysis-nb#parse-data","position":5},{"hierarchy":{"lvl1":"Kinetic analysis notebook","lvl3":"Cytation","lvl2":"Parse Data"},"type":"lvl3","url":"/kinetic-analysis-nb#cytation","position":6},{"hierarchy":{"lvl1":"Kinetic analysis notebook","lvl3":"Cytation","lvl2":"Parse Data"},"content":"\n\n# this should be in a library\nwarnings.filterwarnings(\"ignore\")\n\ndef read_cytation(\n    datafile: str,\n    platemap: pd.DataFrame, \n    sep: str =\"\\t\",\n    blanklabel: str = \"Blank\",\n) -> pd.DataFrame:\n    # read data file as long string\n    data = \"\"\n    with open(datafile, \"r\", encoding=\"latin1\") as file:\n        data = file.read()\n\n    # extract indices for Proc Details, Layout\n    procidx = re.search(r\"Procedure Details\", data)\n    layoutidx = re.search(r\"Layout\", data)\n    readidx = re.search(r\"^(Read\\s)?\\d+,\\d+\", data, re.MULTILINE)\n\n    # get header DataFrame\n    header = data[:procidx.start()]\n    header = pd.read_csv(\n        io.StringIO(header), \n        delimiter=sep, \n        header=0, names=[\"key\", \"value\"]\n    )\n\n    # get procedure DataFrame\n    procedure = data[procidx.end():layoutidx.start()]  \n    procedure = pd.read_csv(io.StringIO(procedure), skipinitialspace=True, names=range(4))\n    procedure = procedure.replace(np.nan, \"\")\n\n    # get Cytation plate map from datafile as DataFrame\n    layout = data[layoutidx.end():readidx.start()]\n    layout = pd.read_csv(io.StringIO(layout), index_col=False)\n    layout = layout.set_index(layout.columns[0])\n    layout.index.name = \"Row\"\n\n    # iterate over data string to find individual reads\n    reads = dict()\n    \n    sep = r\"(?:Read\\s\\d+:)?(?:\\s\\d{3},\\d{3}(?:\\[\\d\\])?)?\" + sep\n        \n    for readidx in re.finditer(r\"^(Read\\s)?\\d+,\\d+.*\\n\", data, re.MULTILINE):\n        # for each iteration, extract string from start idx to end icx\n        read = data[readidx.end():]\n        read = read[:re.search(r\"(^(Read\\s)?\\d+,\\d+|^Blank Read\\s\\d|Results|Max V|\\Z)\", read[1:], re.MULTILINE).start()]\n        read = pd.read_csv(io.StringIO(read), sep=sep, engine=\"python\")\n        reads[data[readidx.start():readidx.end()].strip()] = read\n\n    # create a DataFrame for each read and process, then concatenate into a large DataFrame\n    read_dataframes = list()\n    for name, r in reads.items():\n        # filter out Cytation calculated kinetic parameters, which are cool, but don't want rn\n        r = r[r.Time.str.contains(\"\\d:\\d{2}:\\d{2}\", regex=True)]\n        \n        # extract meaningful parameters from really big string\n        r = r.melt(id_vars=[\"Time\", \"T°\"], var_name=\"Well\", value_name=\"Data\")\n        r['Row'] = r['Well'].str.extract(r\"([A-Z]+)\")\n        r['Column'] = r['Well'].str.extract(r\"(\\d+)\").astype(int)\n        r['Data'] = r['Data'].replace(\"OVRFLW\", np.nan)\n        r['Data'] = r['Data'].astype(float)\n        r['Read'] = name\n        r['Ex'] = r['Read'].str.extract(r\"(\\d+),\\d+\").astype(int)\n        r['Em'] = r['Read'].str.extract(r\"\\d+,(\\d+)\").astype(int)\n        read_dataframes.append(r)\n\n    data = pd.concat(read_dataframes)\n\n    # add time column to data DataFrame\n    data['Time'] = pd.to_timedelta(data['Time']).astype('timedelta64[s]')\n    data['Seconds'] = data['Time'].map(lambda x: x.total_seconds())\n    data = data.merge(platemap, on=[\"Well\", \"Row\", \"Column\"])\n    data.rename(columns={\"Well_x\": \"Well\"}, inplace=True)\n    \n    # apply blanking, if blanklabel given\n    if blanklabel and blanklabel in data.columns:\n        blanks = data[data['Blank'].notna()].groupby([\"Experiment\", \"Read\", \"Time\"]).agg({\"Data\": \"mean\"}).reset_index()\n        data = data.merge(blanks, on=[\"Experiment\", \"Read\", \"Time\"], suffixes=(\"\", \"Blank\"), how=\"left\")\n        data['DataBlanked'] = data['Data'] - data['DataBlank']\n    \n    return data\n\ndata = read_cytation(\n    datafile=\"./sample-data/pure-dna-sweep.txt\",\n    platemap=platemap,\n)\n\ndata.head()\n\n\n","type":"content","url":"/kinetic-analysis-nb#cytation","position":7},{"hierarchy":{"lvl1":"Kinetic analysis notebook","lvl2":"Analysis"},"type":"lvl2","url":"/kinetic-analysis-nb#analysis","position":8},{"hierarchy":{"lvl1":"Kinetic analysis notebook","lvl2":"Analysis"},"content":"\n\n","type":"content","url":"/kinetic-analysis-nb#analysis","position":9},{"hierarchy":{"lvl1":"Kinetic analysis notebook","lvl3":"Basic Plots","lvl2":"Analysis"},"type":"lvl3","url":"/kinetic-analysis-nb#basic-plots","position":10},{"hierarchy":{"lvl1":"Kinetic analysis notebook","lvl3":"Basic Plots","lvl2":"Analysis"},"content":"","type":"content","url":"/kinetic-analysis-nb#basic-plots","position":11},{"hierarchy":{"lvl1":"Kinetic analysis notebook","lvl4":"Plot all wells","lvl3":"Basic Plots","lvl2":"Analysis"},"type":"lvl4","url":"/kinetic-analysis-nb#plot-all-wells","position":12},{"hierarchy":{"lvl1":"Kinetic analysis notebook","lvl4":"Plot all wells","lvl3":"Basic Plots","lvl2":"Analysis"},"content":"\n\nplt = sns.relplot(\n    data=data,\n    x=\"Seconds\",\n    y=\"Data\",\n    hue=\"Well\",\n    row=\"Row\",\n    col=\"Column\",\n    kind=\"line\"\n);\n\n","type":"content","url":"/kinetic-analysis-nb#plot-all-wells","position":13},{"hierarchy":{"lvl1":"Kinetic analysis notebook","lvl3":"DNA Concentration Sweep","lvl2":"Analysis"},"type":"lvl3","url":"/kinetic-analysis-nb#dna-concentration-sweep","position":14},{"hierarchy":{"lvl1":"Kinetic analysis notebook","lvl3":"DNA Concentration Sweep","lvl2":"Analysis"},"content":"Here, we just want to pull out the experiments corresponding to the DNA concentration experiment\n\ndata_concentration = data[data[\"Experiment\"] == \"DNA Concentration\"]\n\ng = sns.relplot(\n    data=data_concentration,\n    x=\"Seconds\",\n    y=\"DataBlanked\",\n    hue=\"[DNA Template] (ng/uL)\",\n    col=\"DNA Template\",\n    kind=\"line\",\n    height=5,\n    aspect=1\n)\ng.set_ylabels(\"Fluorescence (RFU)\")\ng.set_xlabels(\"Time [sec]\")\n\nfor ax in g.axes.flat:\n    ax.spines['top'].set_visible(True)   # Show top frame\n    ax.spines['right'].set_visible(True) # Show right frame\n    ax.spines['left'].set_visible(True)  # Ensure left frame is visible\n    ax.spines['bottom'].set_visible(True)\n;\n\n\n\n","type":"content","url":"/kinetic-analysis-nb#dna-concentration-sweep","position":15},{"hierarchy":{"lvl1":"Kinetic analysis notebook","lvl4":"Kinetics","lvl3":"DNA Concentration Sweep","lvl2":"Analysis"},"type":"lvl4","url":"/kinetic-analysis-nb#kinetics","position":16},{"hierarchy":{"lvl1":"Kinetic analysis notebook","lvl4":"Kinetics","lvl3":"DNA Concentration Sweep","lvl2":"Analysis"},"content":"\n\ndef sigmoid(x, L, k, x0):\n    return (L / (1 + np.exp(-k*(x-x0))))\n\ndef kinetic_analysis_per_well(\n    data: pd.DataFrame, \n    data_column=\"Data\"\n) -> pd.DataFrame:\n    \n    # make initial guesses for parameters\n    L_initial = np.max(data[data_column])\n    x0_initial = np.max(data[\"Seconds\"]) / 4\n    k_initial = (np.log(L_initial * 1.1 / data[data_column] - 1) / (data[\"Seconds\"] - x0_initial)).dropna().mean() * -1.0\n    p0 = [L_initial, k_initial, x0_initial]\n\n    # attempt fitting\n    try:\n        params, _ = curve_fit(sigmoid, data[\"Seconds\"], data[data_column], p0=p0)\n\n        # Get the fitted curve\n        x_fit = data[\"Seconds\"]\n        y_fit = sigmoid(x_fit, *params)\n\n        # calculate velocities and velocity params\n        v = data[data_column].diff(3) / data[\"Seconds\"].diff(3)\n        maxV = v.max()\n        maxV_d = data.loc[v.idxmax(), data_column]\n        maxV_s = data.loc[v.idxmax(), \"Seconds\"]\n\n        # calculate lag time\n        lag = -maxV_d / maxV + maxV_s\n\n        decile_upper = data[data_column].quantile(0.95)\n        decile_lower = data[data_column].quantile(0.05)\n\n        growth_s = (decile_upper - maxV_d) / maxV + maxV_s\n\n        ss_s = data.loc[(data[data_column] > decile_upper).idxmax(), \"Seconds\"]\n        ss_d = data.loc[(data[data_column] > decile_upper).idxmax():, data_column].mean()\n\n        kinetics = {\n            f\"{data_column}_fit_d\": y_fit,\n            f\"{data_column}_maxV\": maxV,\n            f\"{data_column}_maxV_s\": maxV_s,\n            f\"{data_column}_maxV_d\": maxV_d,\n            f\"{data_column}_lag_s\": lag,\n            f\"{data_column}_growth_s\": growth_s,\n            f\"{data_column}_ss_s\": ss_s,\n            f\"{data_column}_ss_d\": ss_d,\n            f\"{data_column}_low_d\": decile_lower,\n            f\"{data_column}_high_d\": decile_upper,\n        }\n\n        return pd.concat([data, pd.DataFrame(kinetics)], axis=1) \n        # return kinetics\n\n    # if Exception raised, exit gracefully(ish)\n    except Exception as e:\n        print(f\"Failed to solve:\")\n        print(e)\n        \n        return None\n\ndef kinetic_analysis(\n    data: pd.DataFrame, \n    data_column=\"Data\",\n    time_cutoff: int = 12000\n) -> pd.DataFrame:\n    \n    tk = (\n        data[(data[\"Seconds\"] < time_cutoff) & (data[\"Blank\"].isna())]\n        .groupby([\"Well\"])\n        .apply(\n            partial(kinetic_analysis_per_well, data_column=data_column)\n        )\n        .reset_index(drop=True)\n    )\n    return tk\n\ndef kinetic_analysis_summary(\n    data: pd.DataFrame,\n    data_column=\"Data\",\n    time_cutoff: int = 12000,\n    label_order: list[str] = None,\n):\n    def per_well_cleanup(df):\n        cols = df.columns\n        return df[[\"Well\"] + list(cols[27:])].aggregate(lambda x: x.iloc[0])\n        \n    tk = kinetic_analysis(data=data, data_column=data_column, time_cutoff=time_cutoff)\n    out = tk.groupby(\"Well\").apply(per_well_cleanup).reset_index(drop=True)\n\n    if label_order:\n        out = out.set_index(\"Well\").reindex(label_order).reset_index()\n    \n    return out\n\ndef plot_kinetics(\n    data: pd.DataFrame, \n    x: str =\"Seconds\", y: str =\"Data\", \n    show_fit: bool = False, show_velocity: bool = False, annotate: bool = False, \n    **kwargs\n):\n    \"\"\"\n    Typical usage:\n    \n    > tk = kinetic_analysis(data=data, data_column=\"BackgroundSubtracted\")\n    > g = sns.FacetGrid(tk, col=\"Well\", col_wrap=2, sharey=False, height=4, aspect=1.5)\n    > g.map_dataframe(plot_kinetics, show_fit=True, show_velocity=True)\n    \"\"\"\n    colors = sns.color_palette(\"Set2\")\n\n    summary = data.iloc[0]\n    \n    ax = sns.scatterplot(\n        data=data, \n        x=x, \n        y=y,\n        alpha=0.5\n    )\n\n    ax_ylim = ax.get_ylim() # Use this to run lines to bounds later, then restore them before returning.\n\n    if show_fit:\n        sns.lineplot(\n            data = data,\n            x = x,\n            y = y,\n            linestyle = \"--\",\n            c = \"red\",\n            alpha=0.5\n        )\n\n    # Max Velocity\n    maxV_x = np.linspace(data[x].min(), data[x].max(), 100)\n    maxV_y = summary[f\"{y}_maxV\"] * (maxV_x - summary[f\"{y}_maxV_s\"]) + summary[f\"{y}_maxV_d\"]\n\n    sns.lineplot(\n        x = maxV_x[(maxV_y > 0) & (maxV_y < data[y].max())],\n        y = maxV_y[(maxV_y > 0) & (maxV_y < data[y].max())],\n        linestyle=\"--\",\n        c=\"r\",\n        ax=ax\n    )\n\n    maxV = summary[f\"{y}_maxV\"]\n    maxV_s = summary[f\"{y}_maxV_s\"]\n    maxV_d = summary[f\"{y}_maxV_d\"]\n\n    # Lag Time\n    lag = summary[f\"{y}_lag_s\"]\n    decile_upper = summary[f\"{y}_high_d\"]\n    decile_lower = summary[f\"{y}_low_d\"]\n    ax.vlines(lag, ymin=ax_ylim[0], ymax=decile_lower, colors=colors[2], linestyle=\"--\")\n\n    # Time to Steady State\n    ss_s = summary[f\"{y}_ss_s\"]\n    ax.axvline(ss_s, c=colors[3], linestyle=\"--\")\n\n    # Range\n    ax.axhline(decile_upper, c=colors[7], linestyle=\"--\")\n    ax.axhline(decile_lower, c=colors[7], linestyle=\"--\")\n\n    if annotate:\n        # Plot the text annotations on the chart\n        ax.annotate(f\"$V_{{max}} = {maxV:.2f} u/s$\", (maxV_s, maxV_d), xytext=(24,0), textcoords=\"offset points\", arrowprops={\"arrowstyle\": \"->\"}, ha=\"left\", va=\"center\", c=\"black\")\n        ax.annotate(f\"$t_{{lag}} = {lag:.0f}$ s\", (lag, decile_lower), xytext=(12, 0), textcoords=\"offset points\", ha=\"left\", va=\"center\")\n        ax.annotate(f\"$t_{{steady state}} = {ss_s - lag:.0f}$ s\", (lag + (ss_s - lag)/4, decile_upper), xytext=(0, -12), textcoords=\"offset points\", ha=\"center\")\n\n    # Velocity\n    if show_velocity:\n        # Show a velocity sparkline over the plot\n        velocity = data.transform({y: \"diff\", x: lambda x: x}).rolling(5).mean()\n        velocity[y] = velocity[y]\n        # velocity_ax = ax.secondary_yaxis(location=\"right\", functions=(lambda x: pd.Series(x).rolling(5).mean().values, lambda x: x))\n        velocity_ax = ax.twinx()\n        sns.lineplot(\n            data=velocity, \n            x=x, \n            y=y,\n            alpha=0.5,\n            ax=velocity_ax\n        )\n        velocity_ax.set_ylabel(\"$V (u/s)$\")\n        velocity_ax.set_ylim((0, velocity[y].max()*2))\n\n    ax.set_ylim(ax_ylim)\n\nwarnings.filterwarnings(\"ignore\")\n\nkinetics = kinetic_analysis(data=data_concentration, data_column=\"Data\", time_cutoff=15000)\n\ng = sns.FacetGrid(kinetics, col=\"Well\", col_wrap=2, sharey=False, height=4, aspect=1.5)\ng.map_dataframe(plot_kinetics, y=\"Data\", show_fit=True, show_velocity=False, annotate=True)\n\nwarnings.filterwarnings(\"ignore\")\n\nsingle_well = data_concentration[data_concentration[\"Well\"] == \"B1\"]\n\nsingle_well_kinetics = kinetic_analysis(data=single_well, data_column=\"Data\", time_cutoff=15000)\ng = sns.FacetGrid(single_well_kinetics, col=\"Well\", col_wrap=2, sharey=False, height=4, aspect=1.5)\ng.map_dataframe(plot_kinetics, y=\"Data\", show_fit=True, show_velocity=False, annotate=True)\n\nplt.show()\n\nkinetic_parameters = [\"Data_lag_s\", \"Data_maxV\", \"Data_ss_d\"]\n\n# Dictionary to store well data\nwell_dict = {}\n\n# Process each well\nfor well in wells:\n    single = kinetics[kinetics['Well'] == well]\n    \n    # Create a dictionary for each well\n    well_dict[well] = {\n        param: single[param].iloc[0] if single[param].nunique() == 1 else 0 \n        for param in kinetic_parameters\n    }\n\n# Convert the well_dict into a DataFrame\nkinetic_params_extract = pd.DataFrame.from_dict(well_dict, orient='index').reset_index()\nkinetic_params_extract = kinetic_params_extract.rename(columns={'index': 'Well'})\n\n\nplatemap_simple = platemap[platemap[\"Experiment\"]==\"DNA Concentration\"].drop(columns=['Experiment', 'Label','Blank', 'Row', 'Column'])\nmerged_df = pd.merge(platemap_simple, kinetic_params_extract, on='Well', how='inner')\n\n# for cleaner visualization\nmerged_df.set_index('Well', drop=True, inplace=False)","type":"content","url":"/kinetic-analysis-nb#kinetics","position":17},{"hierarchy":{"lvl1":"Time course measurements"},"type":"lvl1","url":"/kinetic-analysis-note","position":0},{"hierarchy":{"lvl1":"Time course measurements"},"content":"A shared understanding of how to interpret data from CFE systems would speed the development of measurements and standards towards improved reproducibility and address challenges in interpreting and comparing existing and future data, including data from failed experiments. Workshop participants agreed that time-course measurements of the product expressed in the CFE reaction should be favored over endpoint measurements, despite the time, labor, and costs involved, to obtain a more complete and informative view of a CFE reaction. Whenever possible, measurements should be reported as a reduced quantity, such as a mean value, with uncertainty (Figure 2A) and include a baseline from negative control measurements. Several recent studies have commented on metrics available from a time-course measurement of protein expression in a CFE workflow, for example, for the purpose of reaction optimization [ref] and the development of predictive modeling tools [ref]. Workshop participants emphasized several of these metrics, displayed below in Figure 2\n\nMaximum yield of product expressed;\n\nt_{max} Time to reach the maximum yield of product expressed, as the time from the start of the measurement to the time to reach the maximum yield;\n\n v_{max} Maximum rate of product expression, as the maximum linear rate of production;\n\nLag time, as the time from the start of the measurement to the time to reach the maximum rate of expression;\n\nInflection time, as the time from the start of the measurement to the time at which the rate of product expression begins to decrease;\n\nPercent decline, as a decrease in the amount of product expressed from the maximum yield; and,\n\nTime to percent decline, as the amount of time from the start of the measurement to the time to reach a predefined decrease in the yield of product expressed after having reached its maximum yield of product expressed.","type":"content","url":"/kinetic-analysis-note","position":1},{"hierarchy":{"lvl1":"Time course measurements","lvl2":"Workflow"},"type":"lvl2","url":"/kinetic-analysis-note#workflow","position":2},{"hierarchy":{"lvl1":"Time course measurements","lvl2":"Workflow"},"content":"","type":"content","url":"/kinetic-analysis-note#workflow","position":3},{"hierarchy":{"lvl1":"Time course measurements","lvl3":"Interactivity","lvl2":"Workflow"},"type":"lvl3","url":"/kinetic-analysis-note#interactivity","position":4},{"hierarchy":{"lvl1":"Time course measurements","lvl3":"Interactivity","lvl2":"Workflow"},"content":"import ipywidgets as widgets\nfrom IPython.display import display\n\n# Create a text input widget\ntext_input = widgets.Text(\n    value='World',\n    description='Say:',\n    disabled=False\n)\n\n# Create a function to update the greeting\ndef greet(change):\n    print(f'Hello, {change[\"new\"]}!')\n\n# Attach the function to the text input widget\ntext_input.observe(greet, names='value')\n\n# Display the widget\ndisplay(text_input)","type":"content","url":"/kinetic-analysis-note#interactivity","position":5},{"hierarchy":{"lvl1":"Time course measurements","lvl3":"Platemap","lvl2":"Workflow"},"type":"lvl3","url":"/kinetic-analysis-note#platemap","position":6},{"hierarchy":{"lvl1":"Time course measurements","lvl3":"Platemap","lvl2":"Workflow"},"content":"Note\n\nIdeally, we would have a tool that (1) let’s people describe experiments in a standardized way (spec lang), (2) that can then be mapped onto a platemap, and (3) displayed in part or whole in this section [this might be an interesting place for a widget to explore the parameters of the experiment]\n\nThis is text describing the context of the experiment. There a few scenarios where this text can end up here. We can imagine users working in jupyterhub typing directly or its copy and pasted in from an experimental notebook. In this particular case, we have two experiments on testing the effect of DNA concentration and a second experiment comparing different DNA constrcuts - the infamous artifact 11 (AR-11)\n\nTable 1:Experimental conditions studied","type":"content","url":"/kinetic-analysis-note#platemap","position":7},{"hierarchy":{"lvl1":"Time course measurements","lvl3":"Analysis","lvl2":"Workflow"},"type":"lvl3","url":"/kinetic-analysis-note#analysis","position":8},{"hierarchy":{"lvl1":"Time course measurements","lvl3":"Analysis","lvl2":"Workflow"},"content":"One thing that might be interesting to show here would be a progression from end-point analysis, unannotated timeseries, time series annotations, and then exploring the kinetic parameters.\n\n# experiment -> \"DNA Concentration\"\ndata_concentration = data[data[\"Experiment\"] == \"DNA Concentration\"]\n\nProgram 1:filter the plate map to select for a specific experiment\n\nHere, \n\nProgram 1 is used to generate the following figure using these \n\nspecific lines of code from the cytosol analysis toolkit. This might be useful for instance if we’re showcasing a new feature in the codebase and we want to make a tight connection between code and function, inviting engagement.\n\nThe immediate advantage of this notebook over our existing way of publishing is that the figure is unambiguously linked to the data and code used to generate it. In this particular case, the sample data is co-located in the directory used to generate the HTML. Another approach might be to pull the data programmatically from an archive like zenodo.\n\nPulling data directly from an archive# this workflow would also work for OSF\nfrom zenodo_client import Zenodo\n\nzenodo = Zenodo(access_token=token)\nOOH_NA_NA_RECORD = '13852103'\n\nplatemap_file = '20240916-platemap.xlsx'\ndata_file = 'timecourse.csv'\n\ndata = zenodo.download_latest(OOH_NA_NA_RECORD, data_file)\nplatemap = zenodo.download_latest(OOH_NA_NA_RECORD, platemap_file)\n\n\n\nFigure 1:The plasmid pT7-deGFP (AR-11) is swept from 0-100 ng/μL in the PURE system\n\nNow, we can showcase the kinetic analysis tool by showing a single timeseries selected from the above experiment where the extracted features are annotated. It was a bit annoying to extract one plot from the FacetGrid. Instead, I reduced the DataFrame\n\nPlotting a single annotated kinetic tracewarnings.filterwarnings(\"ignore\")\n\nsingle_well = data_concentration[data_concentration[\"Well\"] == \"B1\"]\n\nsingle_well_kinetics = kinetic_analysis(data=single_well, data_column=\"Data\", time_cutoff=15000)\ng = sns.FacetGrid(single_well_kinetics, col=\"Well\", col_wrap=2, sharey=False, height=4, aspect=1.5)\ng.map_dataframe(plot_kinetics, y=\"Data\", show_fit=True, show_velocity=False, annotate=True)\n\n\n\nFigure 2:Annotated timeseries for pT7-deGFP (AR-11) in PURE at a concentration of 100 ng/μL\n\nOf course, the figure isn’t really what’s valuable. Ultimately, we want to associate the dimension-reduced kinetic parameters with the experimental specification. We can push this to a database like SeqBase where the input sequence and experimental specification are coupled to the output kinetic parameters.\n\nOther types of data for non-fluorescent data will have to be developed over time.\n\nTable 1:Combined experimental specification with kinetic parameters extracted from data.","type":"content","url":"/kinetic-analysis-note#analysis","position":9}]}